#coding:utf-8
from skimage.measure import label
from PIL import Image
import numpy as np
import torch
import os.path as osp
import cv2
import os
import multiprocessing as mp
from torch.nn import functional as F
import argparse

def parse_args():
    parser = argparse.ArgumentParser('fuse two model results on target dataset')
    parser.add_argument('--Edge_path',type=str,help='the refined edge maps path')
    parser.add_argument('--SoftSegMap_path1',type=str,help='the folder path of the soft-segmentation maps generated by the first model on the target dataset')
    parser.add_argument('--SoftSegMap_path2',type=str,help='the folder path of the soft-segmentation maps generated by the second model on the target dataset')
    parser.add_argument('--save_file',type=str,default='result_fuse')
    parser.add_argument('--classes',type=int,help='number of semantic categories')
    return parser.parse_args()

# pallete = [[128, 64, 128],
#            [244, 35, 232],
#            [70, 70, 70],
#            [102, 102, 156],
#            [190, 153, 153],
#            [153, 153, 153],
#            [250, 170, 30],
#            [220, 220, 0],
#            [107, 142, 35],
#            [152, 251, 152],
#            [70, 130, 180],
#            [220, 20, 60],
#            [255, 0, 0],
#            [0, 0, 142],
#            [0, 0, 70],
#            [0, 60, 100],
#            [0, 80, 100],
#            [0, 0, 230],
#            [119, 11, 32],
#            [128,0,0],
#            [128,128,0],
#            [0,0,128],
#            [128,0,128],
#            [64,0,0],
#            [192,0,0],
#            [64,128,0],
#            [192,128,0],
#            [64,0,128],
#            [192,0,128],
#            [0,64,0],
#            [128,64,0],
#            [0,192,0],
#            [0,64,128],
#            [0, 0, 0]]

def normalized(x):
    max=x.max()
    min=x.min()
    x_=(x-min)/(max-min)
    return x_
def diff(x):
    x_=sorted(x,reverse=True)
    x_t=torch.tensor(x_,dtype=torch.float)
    x_g=normalized(x_t)
    x_cha=x_g[0]-x_g[1]
    return x_cha

def fuse_2maps(label,num,img1,pro1,img2,pro2,c1,c2,c):
    use_1=0
    use_2=0
    h,w=img1.shape

    pro1_torch = torch.from_numpy(pro1)
    pro2_torch = torch.from_numpy(pro2)

    # compute information entropy
    l1=pro1_torch.mul(torch.log(pro1_torch))
    l2=pro2_torch.mul(torch.log(pro2_torch))

    np_result=np.full((h,w),c)

    for i in range(num):
        if i == 0 :
            # i=0 means the pixel is an edge point
            continue
        else:
        # count the classification results with the most votes in each image block
            list1=[0 for x in range(c1)]
            list2=[0 for y in range(c2)]
            for x1 in range(c1):
                list1[x1] = sum(img1[label == i] == x1)
            for y1 in range(c2):
                list2[y1] = sum(img2[label == i] == y1)

            # determine whether the image block is correctly divided
            diff1=diff(list1)
            diff2=diff(list2)
            if diff1<0.5 or diff2<0.5:
                # use the result of the model with the smaller information entropy
                mask = [label == i][0]
                ind = np.argwhere(mask == True)
                l1_i = 0
                l2_i = 0
                for j in range(len(ind)):
                    l1_i_pro = l1[ind[j][0], ind[j][1], :]
                    l2_i_pro = l2[ind[j][0], ind[j][1], :]
                    l1_i = float(l1_i + torch.sum(l1_i_pro))
                    l2_i = float(l2_i + torch.sum(l2_i_pro))
                l1_i = -l1_i
                l2_i = -l2_i
                if l1_i<l2_i:
                    for k in range(len(ind)):
                        np_result[ind[k][0],ind[k][1]]=img1[ind[k][0],ind[k][1]]
                else:
                    for k in range(len(ind)):
                        np_result[ind[k][0],ind[k][1]]=img2[ind[k][0],ind[k][1]]

            else:
                max1=list1.index(max(list1))
                max2=list2.index(max(list2))
                mask=[label==i][0]

                ind=np.argwhere(mask==True)
                l1_i=0
                l2_i=0
                for j in range(len(ind)):
                    l1_i_pro=l1[ind[j][0],ind[j][1],:]
                    l2_i_pro=l2[ind[j][0],ind[j][1],:]
                    l1_i=float(l1_i+torch.sum(l1_i_pro))
                    l2_i=float(l2_i+torch.sum(l2_i_pro))
                l1_i=-float(l1_i)/float(c1)
                l2_i=-float(l2_i)/float(c2)
                if l1_i<l2_i:
                    np_result[label==i]=max1
                    use_1+=1
                else:
                    np_result[label==i]=max2
                    use_2+=1

    return np_result,use_1,use_2

def text_create(name, msg, save_each_label_path):
    full_path = os.path.join(save_each_label_path,name[:-4]+'.txt')
    file = open(full_path, 'w')
    file.write(msg)
    file.close()

def deal_per_line(i,args):

    if not os.path.exists(args.save_file):
        os.mkdir(args.save_file)
    if not os.path.exists(os.path.join(args.save_file,'int')):
        os.mkdir(os.path.join(args.save_file,'int'))
    if not os.path.exists(os.path.join(args.save_file,'vote')):
        os.mkdir(os.path.join(args.save_file,'vote'))

    # load edge maps and divid the connected valued pixels into a connected component
    edge_img_path=os.path.join(args.Edge_path,i)
    edge_img=Image.open(edge_img_path)
    edge_np=np.asarray(edge_img)
    labeled_edge, num = label(edge_np, neighbors=4,background=255, return_num=True)

    # load soft-segmentation maps and semantic segmentation results
    ssm_model1_path=os.path.join(args.SoftSegMap_path1,i)
    ssr_model1_path=ssm_model1_path[:-4]+'.npy'
    ssm_model1_np=np.load(ssm_model1_path)
    _,_,c1=ssm_model1_np.shape

    ssm_model2_path=os.path.join(args.SoftSegMap_path2,i)
    ssr_model2_path=ssm_model2_path[:-4]+'.npy'
    ssm_model2_np=np.load(ssm_model2_path)
    _,_,c2=ssm_model2_np.shape

    ssr_model1_img = Image.open(ssr_model1_path)
    ssr_model1_img = np.array(ssr_model1_img)

    ssr_model2_img = Image.open(ssr_model2_path)
    ssr_model2_img = np.array(ssr_model2_img)

    # fuse two maps
    classes=args.classes
    fuse_img,use_model1,use_model2=fuse_2maps(labeled_edge,num,ssm_model1_np,ssr_model1_img,ssm_model2_np,ssr_model2_img,c1,c2,classes)

    # visualization
    # fuse_color=np.zeros((height,width,3),dtype=np.uint8)
    # for idx in range(len(pallete)):
    #     [r, g, b] = pallete[idx]
    #     fuse_color[fuse_img == idx] = [b, g, r]

    save_int_path=osp.join(args.save_file,'int',i)
    cv2.imwrite(save_int_path,fuse_img)

    print(i+' use model1 result:', use_model1,',','use model2 result:', use_model2)
    save_msg='use model1 label:'+str(use_model1)+','+'use model2 label:'+str(use_model2)

    vote_file_path=os.path.join(args.save_file,'vote')
    text_create(i,save_msg,vote_file_path)

def main():
    args=parse_args()
    edge_path=args.Edge_path
    f=os.listdir(edge_path)
    for i in f:
        deal_per_line(i,args)

if __name__ == '__main__':
    main()
